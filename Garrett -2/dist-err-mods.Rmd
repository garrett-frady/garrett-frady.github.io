---
title: "Distance Error Models"
date: "`r Sys.Date()`"
author: "Garrett Frady and Tim Moore"
output:
  rmdformats::downcute:
    code_folding: hide
    self_contained: true
    default_style: "light"
    downcute_theme: "default"
editor_options: 
  chunk_output_type: console
---


```{r setup, include=FALSE}
## Global options
knitr::opts_chunk$set(cache = TRUE,
                      echo = T,
                      eval = T,
                      warning = F,
                      message = F)

library(dplyr) # manipulate data
library(ordbetareg) # ordered beta regression; Bayesian model
library(modelsummary) # summary of model fit
library(marginaleffects) # extract marginal effects from model fit
library(lme4)

```

```{r}
# need to change the file name to match your path destination or change working directory
# setwd(dir = "Andys_Project_Info/")
dat <- read.csv("Master FPL Summary Data.csv")

# transform percents to proportions so values are between 0 and 1
dat$dist_err_propoff <- dat$dist_err_percentoff/100
```

# Experiment 1

```{r exp1, results=FALSE}
library(lmerTest)
# EXPERIMENT 1
# same predictors as logistic regression models for success, with sequence included
#hist(dat$total_years_experience)

mod.1 <- lmer(dist_err_propoff ~ trial_type*(path + total_years_experience) +
                trial_across_both_experiments +
                             (1|participant_number), 
                           data = dat %>% dplyr::filter(experiment == 1))

################### Model Summary ###################
# smmary of the model fit above
modsum1 <- summary(mod.1)

# additional summary table of the model fit above
sumtab1 <- sjPlot::tab_model(mod.bayesian1)

################# Marginal Effects ##################
marg_effs1 <- marginaleffects(mod.bayesian1)

################ Posterior Estimation ################
all_draws1 <- prepare_predictions(mod.bayesian1)

# extract the model cutpoints and overlay them on the empirical distribution to ... 
# ... see how the model is dividing the outcome into discrete-ish categories
# need totransform cutpoints using inverse logit function
cutzero1 <- plogis(all_draws1$dpars$cutzero)
cutone1 <- plogis(all_draws1$dpars$cutzero + exp(all_draws1$dpars$cutone))

# histograms of the cutpoints above; commented out as they are not necessary ...
# ... but I left them in case we were eventually interested
# hist(cutzero1)
# hist(cutone1)

# distribution of distance error proportion off including mean cutpoints as dotted lines
dist_err1_plot <- dat %>% 
  ggplot(aes(x=dist_err_propoff)) +
  geom_histogram(bins=20) +
  theme_minimal() + 
  theme(panel.grid=element_blank()) +
  scale_x_continuous(breaks=c(0, 0.25, 0.5, 0.75, 1)) +
  geom_vline(xintercept = mean(cutzero1),linetype=2) +
  geom_vline(xintercept = mean(cutone1),linetype=2) +
  xlim(c(0, 1)) +
  ylim(c(0, 20)) +
  ylab("") +
  xlab("") +
  labs(caption=paste0("Figure shows the distribution of  distance error proportion off."))

# posterior predictions
pred_post1 <- posterior_predict(mod.bayesian1, mod.bayesian1$dist_err_propoff)

# The discrete plot, which is a bar graph, shows that the posterior distributional...
# ... accurately captures the number of discrete responses in the data. For the ...
# continuous plot, shown as a density plot with one line per posterior draw, the ...
# model can’t capture all of the modality in the distribution – there are ...
# effectively four separate modes – but it is reasonably accurate over the middle ...
# responses and the responses near the bounds.
plots1 <- pp_check_ordbeta(mod.bayesian1, ndraws = 100)
```

## Model Fit

A summary of the model fit is given in the table below, along with an additional summary table. They contain similar information, so we can remove one eventually. 

```{r}
modsum1
sumtab1
```

## Marginal Effects

```{r}
summary(marg_effs1) %>% knitr::kable()
```

## Posterior Estimation

### Posterior Distribution

Below is a histogram representing the empirical distribution of distance error proportion off, including mean cutpoints as dotted lines, to see how the model divides the outcome into discrete-ish categories. 

```{r}
dist_err1_plot
```

### Posterior Prediction

Below is a table representing the posterior estimates of dist_err_propoff for each observation in experiment 1. These are claculated from the posterior predictive distribution, and by averaging over all iterations (or draws). 

```{r}
colMeans(pred_post1)
```

The mean of all posterior estimates is shown below.

```{r}
mean(colMeans(pred_post1))
```


## Posterior Checking 

The discrete plot, which is a bar graph, shows that the posterior distributional accurately captures the number of discrete responses in the data. For the continuous plot, shown as a density plot with one line per posterior draw, the model can’t capture all of the modality in the distribution. The two plots are included below. 

```{r}
plots1$discrete
plots1$continuous
```

# Experiment 2

```{r exp2, results=FALSE}
# EXPERIMENT 2
# same predictors as logistic regression models for success, with sequence included
mod.bayesian2 <- ordbetareg(dist_err_propoff ~ trial_type*path + trial_across_both_experiments +
                             (1|participant_number), 
                           chains = 1,
                           data = dat %>% dplyr::filter(experiment == 2))

################### Model Summary ###################
# smmary of the model fit above
modsum2 <- modelsummary(mod.bayesian2, statistic = "conf.int", metrics = "RMSE")

# additional summary table of the model fit above
sumtab2 <- sjPlot::tab_model(mod.bayesian2)

################# Marginal Effects ##################
marg_effs2 <- marginaleffects(mod.bayesian2)

################ Posterior Estimation ################
all_draws2 <- prepare_predictions(mod.bayesian2)

# extract the model cutpoints and overlay them on the empirical distribution to ... 
# ... see how the model is dividing the outcome into discrete-ish categories
# need totransform cutpoints using inverse logit function
cutzero2 <- plogis(all_draws2$dpars$cutzero)
cutone2 <- plogis(all_draws2$dpars$cutzero + exp(all_draws2$dpars$cutone))

# histograms of the cutpoints above; commented out as they are not necessary ...
# ... but I left them in case we were eventually interested
# hist(cutzero2)
# hist(cutone2)

# distribution of distance error proportion off including mean cutpoints as dotted lines
dist_err2_plot <- dat %>% 
  ggplot(aes(x=dist_err_propoff)) +
  geom_histogram(bins=20) +
  theme_minimal() + 
  theme(panel.grid=element_blank()) +
  scale_x_continuous(breaks=c(0, 0.25, 0.5, 0.75, 1)) +
  geom_vline(xintercept = mean(cutzero2),linetype=2) +
  geom_vline(xintercept = mean(cutone2),linetype=2) +
  xlim(c(0, 1)) +
  ylim(c(0, 20)) +
  ylab("") +
  xlab("") +
  labs(caption=paste0("Figure shows the distribution of  distance error proportion off."))

# posterior predictions
pred_post2 <- posterior_predict(mod.bayesian2, mod.bayesian2$dist_err_propoff)

# The discrete plot, which is a bar graph, shows that the posterior distributional...
# ... accurately captures the number of discrete responses in the data. For the ...
# continuous plot, shown as a density plot with one line per posterior draw, the ...
# model can’t capture all of the modality in the distribution – there are ...
# effectively four separate modes – but it is reasonably accurate over the middle ...
# responses and the responses near the bounds.
plots2 <- pp_check_ordbeta(mod.bayesian2, ndraws = 100)
```

## Model Fit

A summary of the model fit is given in the table below, along with an additional summary table. They contain similar information, so we can choose one eventually. 

```{r}
modsum2
sumtab2
```

## Marginal Effects

```{r}
summary(marg_effs2) %>% knitr::kable()
```

## Posterior Estimation

### Posterior Distribution

Below is a histogram representing the empirical distribution of distance error proportion off, including mean cutpoints as dotted lines, to see how the model divides the outcome into discrete-ish categories. 

```{r}
dist_err2_plot
```

### Posterior Prediction

Below is a table representing the posterior estimates of dist_err_propoff for each observation in experiment 2. These are claculated from the posterior predictive distribution, and by averaging over all iterations (or draws). 

```{r}
colMeans(pred_post2)
```

The mean of all posterior estimates is shown below.

```{r}
mean(colMeans(pred_post1))
```


## Posterior Checking 

The discrete plot, which is a bar graph, shows that the posterior distributional accurately captures the number of discrete responses in the data. For the continuous plot, shown as a density plot with one line per posterior draw, the model can’t capture all of the modality in the distribution. The two plots are included below. 

```{r}
plots2$discrete
plots2$continuous
```


# New Boxplots using Posterior Predictions for each individual

```{r}
# adding columns representing the posterior predictions of dist_err_propoff ...
# ... for each subject; one for experiment 1 and one for experiment 2
dat$posterior_predictions1 <- colMeans(pred_post1)
dat$posterior_predictions2 <- colMeans(pred_post2)
```

## Experiment 1

```{r}
dist_err_propoff_boxplot1 <- dat %>% filter(experiment == 1) %>%
  ggplot(aes(x = path, y = posterior_predictions1)) +
  geom_boxplot() + 
  geom_point(aes(color = trial_type, shape = success), size = 3)

dist_err_propoff_boxplot1a <- dat %>% filter(experiment == 1) %>%
  ggplot(aes(x = path, y = posterior_predictions1, fill = trial_type)) +
  geom_boxplot() +
  facet_wrap(~trial_type) + 
  geom_point(aes(shape = success), size = 3)

dist_err_propoff_boxplot1b <- dat %>% filter(experiment == 1) %>%
  ggplot(aes(x = path, y = posterior_predictions1, fill = trial_type)) +
  geom_boxplot() 

dist_err_propoff_boxplot1c <- dat %>% filter(experiment == 1, success == "yes") %>%
  ggplot(aes(x = path, y = posterior_predictions1, fill = trial_type)) +
  geom_boxplot()

dist_err_propoff_boxplot1d <- dat %>% filter(experiment == 1, success == "no") %>%
  ggplot(aes(x = path, y = posterior_predictions1, fill = trial_type)) +
  geom_boxplot()
```

Boxplots of posterior predictions of dist_err_propoff for each subject, separated by path. The color of the points represent the trial_type (belt vs. no belt) and the shapes represent the outcome of the trial (success = yes/no). 

```{r}
dist_err_propoff_boxplot1
```

Now the boxplots are also separated by trial_type (belt vs no belt).

```{r}
dist_err_propoff_boxplot1a
```

LIKELY THE MAIN PLOT OF INTEREST

This plot is similar to the plot above, but we have further grouped the boxplots by trial_type for easier comparisons at each path. We also do not include the points regarding success (yes/no).

```{r}
dist_err_propoff_boxplot1b
```

The grid below contains two plots similar to the one above, however, they are separated based on success (yes/no). The plot on the left is for successful trials and the plot on the right is for failed trials. 

```{r}
cowplot::plot_grid(dist_err_propoff_boxplot1c, dist_err_propoff_boxplot1d, labels = c("Yes", "No"), vjust = 1)
```

## Experiment 2

```{r}
dist_err_propoff_boxplot2 <- dat %>% filter(experiment == 2) %>%
  ggplot(aes(x = path, y = posterior_predictions2)) +
  geom_boxplot() + 
  geom_point(aes(color = trial_type, shape = success), size = 3)

dist_err_propoff_boxplot2a <- dat %>% filter(experiment == 2) %>%
  ggplot(aes(x = path, y = posterior_predictions2, fill = trial_type)) +
  geom_boxplot() +
  facet_wrap(~trial_type)

dist_err_propoff_boxplot2b <- dat %>% filter(experiment == 2) %>%
  ggplot(aes(x = path, y = posterior_predictions2, fill = trial_type)) +
  geom_boxplot() 

dist_err_propoff_boxplot2c <- dat %>% filter(experiment == 2, success == "yes") %>%
  ggplot(aes(x = path, y = posterior_predictions2, fill = trial_type)) +
  geom_boxplot()

dist_err_propoff_boxplot2d <- dat %>% filter(experiment == 2, success == "no") %>%
  ggplot(aes(x = path, y = posterior_predictions2, fill = trial_type)) +
  geom_boxplot()
```

Boxplots of posterior predictions of dist_err_propoff for each subject, separated by path. The color of the points represent the trial_type (belt vs. no belt) and the shapes represent the outcome of the trial (success = yes/no). 

```{r}
dist_err_propoff_boxplot2
```

Now the boxplots are also separated by trial_type (belt vs no belt).

```{r}
dist_err_propoff_boxplot2a
```


LIKELY THE MAIN PLOT OF INTEREST 

This plot is similar to the plot above, but we have further grouped the boxplots by trial_type for easier comparisons at each path. We also do not include the points regarding success (yes/no).

```{r}
dist_err_propoff_boxplot2b
```

The grid below contains two plots similar to the one above, however, they are separated based on success (yes/no). The plot on the left is for successful trials and the plot on the right is for failed trials. 

```{r}
cowplot::plot_grid(dist_err_propoff_boxplot2c, dist_err_propoff_boxplot2d, labels = c("Yes", "No"), vjust = 1)
```
